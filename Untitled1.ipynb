{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30fa81cc-5836-41a2-a886-c1716cff0849",
   "metadata": {},
   "source": [
    "To automatically create or extract events in the form of a JSON or formatted dictionary from paragraphs in a report (like the Beige Book), you can follow a structured approach. This involves using Natural Language Processing (NLP) techniques to identify significant events, their details, and their timing. Below is a step-by-step guide:\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 1: Preprocess the Text**\n",
    "1. **Tokenization**: Split the text into sentences or tokens.\n",
    "2. **Part-of-Speech (POS) Tagging**: Identify nouns, verbs, and other parts of speech to understand the structure of the text.\n",
    "3. **Named Entity Recognition (NER)**: Detect entities like dates, locations, organizations, and people.\n",
    "4. **Dependency Parsing**: Understand the relationships between words in a sentence.\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 2: Identify Key Information**\n",
    "1. **Event Detection**:\n",
    "   - Look for verbs or phrases that indicate significant actions or changes (e.g., \"increased,\" \"declined,\" \"announced\").\n",
    "   - Use keyword matching or machine learning models to detect events.\n",
    "2. **Temporal Information**:\n",
    "   - Extract dates, times, or relative time references (e.g., \"last month,\" \"in Q2\").\n",
    "3. **Entities Involved**:\n",
    "   - Identify organizations, people, or locations associated with the event.\n",
    "4. **Contextual Details**:\n",
    "   - Extract additional details like reasons, impacts, or outcomes of the event.\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 3: Structure the Data**\n",
    "Once the key information is extracted, structure it into a JSON or dictionary format. Here’s an example schema:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"event_id\": \"unique_id\",\n",
    "  \"event_description\": \"A brief description of the event\",\n",
    "  \"date\": \"YYYY-MM-DD or relative time\",\n",
    "  \"entities_involved\": {\n",
    "    \"organizations\": [],\n",
    "    \"people\": [],\n",
    "    \"locations\": []\n",
    "  },\n",
    "  \"context\": {\n",
    "    \"reason\": \"Why the event happened\",\n",
    "    \"impact\": \"What was the result or effect\",\n",
    "    \"additional_details\": \"Any other relevant information\"\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 4: Automate the Process**\n",
    "1. **Rule-Based Approach**:\n",
    "   - Use regular expressions and predefined rules to extract events and their details.\n",
    "   - Example: Extract sentences containing specific keywords like \"increase,\" \"decline,\" or \"announcement.\"\n",
    "\n",
    "2. **Machine Learning Approach**:\n",
    "   - Train a model (e.g., using spaCy, Hugging Face Transformers) to classify sentences as events and extract relevant details.\n",
    "   - Use pre-trained models for NER and event detection.\n",
    "\n",
    "3. **Hybrid Approach**:\n",
    "   - Combine rule-based methods with machine learning for better accuracy.\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 5: Example Implementation**\n",
    "Here’s a Python example using spaCy for event extraction:\n",
    "\n",
    "```python\n",
    "import spacy\n",
    "from datetime import datetime\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Sample paragraph\n",
    "text = \"\"\"\n",
    "In Q2 2023, the Federal Reserve reported a significant increase in consumer spending. \n",
    "This was driven by strong job growth and rising wages. However, inflation remained a concern.\n",
    "\"\"\"\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(text)\n",
    "\n",
    "# Extract events\n",
    "events = []\n",
    "for sent in doc.sents:\n",
    "    event = {\n",
    "        \"event_description\": sent.text,\n",
    "        \"date\": \"Q2 2023\",  # Extracted using NER or regex\n",
    "        \"entities_involved\": {\n",
    "            \"organizations\": [ent.text for ent in sent.ents if ent.label_ == \"ORG\"],\n",
    "            \"people\": [ent.text for ent in sent.ents if ent.label_ == \"PERSON\"],\n",
    "            \"locations\": [ent.text for ent in sent.ents if ent.label_ == \"GPE\"]\n",
    "        },\n",
    "        \"context\": {\n",
    "            \"reason\": \"strong job growth and rising wages\",\n",
    "            \"impact\": \"significant increase in consumer spending\",\n",
    "            \"additional_details\": \"inflation remained a concern\"\n",
    "        }\n",
    "    }\n",
    "    events.append(event)\n",
    "\n",
    "# Output as JSON\n",
    "import json\n",
    "print(json.dumps(events, indent=2))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 6: Refine and Validate**\n",
    "1. **Refinement**:\n",
    "   - Use more advanced NLP techniques (e.g., coreference resolution) to improve accuracy.\n",
    "   - Fine-tune models on domain-specific data (e.g., economic reports).\n",
    "2. **Validation**:\n",
    "   - Manually review a sample of extracted events to ensure accuracy.\n",
    "   - Use metrics like precision, recall, and F1-score to evaluate performance.\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 7: Store and Use the Data**\n",
    "- Store the extracted events in a database or a JSON file.\n",
    "- Use the data for tracking significant events, generating summaries, or creating timelines.\n",
    "\n",
    "---\n",
    "\n",
    "### **Tools and Libraries**\n",
    "- **spaCy**: For NLP tasks like tokenization, POS tagging, and NER.\n",
    "- **Hugging Face Transformers**: For advanced language models.\n",
    "- **Regex**: For pattern matching.\n",
    "- **dateparser**: For extracting and parsing dates.\n",
    "\n",
    "By following these steps, you can automate the extraction of significant events from reports and structure them in a usable format like JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01b2f8a0-33ff-4633-96b7-e8f429832fa0",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_21109/2102096607.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Load spaCy model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"en_core_web_sm\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Sample paragraph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/spacy/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mRETURNS\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mLanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mloaded\u001b[0m \u001b[0mnlp\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \"\"\"\n\u001b[0;32m---> 51\u001b[0;31m     return util.load_model(\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m    470\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mOLD_MODEL_SHORTCUTS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE941\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mOLD_MODEL_SHORTCUTS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[index]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 472\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE050\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    473\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from datetime import datetime\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Sample paragraph\n",
    "text = \"\"\"\n",
    "In Q2 2023, the Federal Reserve reported a significant increase in consumer spending. \n",
    "This was driven by strong job growth and rising wages. However, inflation remained a concern.\n",
    "\"\"\"\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(text)\n",
    "\n",
    "# Extract events\n",
    "events = []\n",
    "for sent in doc.sents:\n",
    "    event = {\n",
    "        \"event_description\": sent.text,\n",
    "        \"date\": \"Q2 2023\",  # Extracted using NER or regex\n",
    "        \"entities_involved\": {\n",
    "            \"organizations\": [ent.text for ent in sent.ents if ent.label_ == \"ORG\"],\n",
    "            \"people\": [ent.text for ent in sent.ents if ent.label_ == \"PERSON\"],\n",
    "            \"locations\": [ent.text for ent in sent.ents if ent.label_ == \"GPE\"]\n",
    "        },\n",
    "        \"context\": {\n",
    "            \"reason\": \"strong job growth and rising wages\",\n",
    "            \"impact\": \"significant increase in consumer spending\",\n",
    "            \"additional_details\": \"inflation remained a concern\"\n",
    "        }\n",
    "    }\n",
    "    events.append(event)\n",
    "\n",
    "# Output as JSON\n",
    "import json\n",
    "print(json.dumps(events, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c69c974-aa95-46b5-bfd6-cfc2b3c94e89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1281c340-819a-48ed-99dc-df861bd4a2ec",
   "metadata": {},
   "source": [
    "Creating a vector index from chunks of text extracted from paragraphs involves converting the text into numerical representations (embeddings) and organizing them in a way that allows for efficient similarity search or retrieval. This is commonly used in applications like semantic search, recommendation systems, or question-answering systems.\n",
    "\n",
    "Here’s a step-by-step guide to creating a vector index:\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 1: Install Required Libraries**\n",
    "You’ll need libraries for text processing, embeddings, and vector indexing. Install the following:\n",
    "\n",
    "```bash\n",
    "pip install sentence-transformers faiss-cpu\n",
    "```\n",
    "\n",
    "- **`sentence-transformers`**: For generating text embeddings.\n",
    "- **`faiss`**: For efficient vector indexing and similarity search.\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 2: Generate Text Embeddings**\n",
    "Convert the extracted text chunks into numerical vectors (embeddings) using a pre-trained language model.\n",
    "\n",
    "```python\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load a pre-trained embedding model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Example text chunks (extracted from paragraphs)\n",
    "text_chunks = [\n",
    "    \"In Q2 2023, the Federal Reserve reported a significant increase in consumer spending.\",\n",
    "    \"This was driven by strong job growth and rising wages.\",\n",
    "    \"However, inflation remained a concern.\"\n",
    "]\n",
    "\n",
    "# Generate embeddings for each text chunk\n",
    "embeddings = model.encode(text_chunks)\n",
    "\n",
    "print(\"Embeddings shape:\", embeddings.shape)  # (num_chunks, embedding_dimension)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 3: Create a Vector Index**\n",
    "Use a vector indexing library like FAISS to organize the embeddings for efficient search.\n",
    "\n",
    "```python\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# Get the dimensionality of the embeddings\n",
    "embedding_dim = embeddings.shape[1]\n",
    "\n",
    "# Create a FAISS index\n",
    "index = faiss.IndexFlatL2(embedding_dim)  # L2 distance for similarity search\n",
    "\n",
    "# Add embeddings to the index\n",
    "index.add(np.array(embeddings))\n",
    "\n",
    "print(\"Number of vectors in index:\", index.ntotal)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 4: Perform Similarity Search**\n",
    "You can now query the index to find the most similar text chunks to a given query.\n",
    "\n",
    "```python\n",
    "# Example query\n",
    "query = \"What caused the increase in consumer spending?\"\n",
    "\n",
    "# Generate embedding for the query\n",
    "query_embedding = model.encode([query])\n",
    "\n",
    "# Search the index\n",
    "k = 2  # Number of nearest neighbors to retrieve\n",
    "distances, indices = index.search(query_embedding, k)\n",
    "\n",
    "# Retrieve the most similar text chunks\n",
    "for i, idx in enumerate(indices[0]):\n",
    "    print(f\"Result {i + 1}: {text_chunks[idx]} (Distance: {distances[0][i]})\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 5: Save and Load the Index**\n",
    "To reuse the index, save it to disk and load it later.\n",
    "\n",
    "```python\n",
    "# Save the index\n",
    "faiss.write_index(index, \"vector_index.faiss\")\n",
    "\n",
    "# Load the index\n",
    "loaded_index = faiss.read_index(\"vector_index.faiss\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 6: Advanced Indexing (Optional)**\n",
    "For larger datasets, you can use more advanced FAISS indexes like `IndexIVFFlat` or `IndexHNSW` for better performance.\n",
    "\n",
    "```python\n",
    "# Create a quantized index for faster search\n",
    "nlist = 100  # Number of clusters\n",
    "quantizer = faiss.IndexFlatL2(embedding_dim)\n",
    "advanced_index = faiss.IndexIVFFlat(quantizer, embedding_dim, nlist)\n",
    "\n",
    "# Train the index on a sample of embeddings\n",
    "sample_embeddings = embeddings[:1000]  # Use a subset for training\n",
    "advanced_index.train(sample_embeddings)\n",
    "\n",
    "# Add all embeddings to the index\n",
    "advanced_index.add(embeddings)\n",
    "\n",
    "# Save the advanced index\n",
    "faiss.write_index(advanced_index, \"advanced_vector_index.faiss\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 7: Use Cases**\n",
    "- **Semantic Search**: Find the most relevant text chunks for a given query.\n",
    "- **Clustering**: Group similar text chunks together.\n",
    "- **Recommendation Systems**: Suggest related content based on embeddings.\n",
    "\n",
    "---\n",
    "\n",
    "### **Full Example Code**\n",
    "Here’s the complete code for creating and querying a vector index:\n",
    "\n",
    "```python\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Generate embeddings\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "text_chunks = [\n",
    "    \"In Q2 2023, the Federal Reserve reported a significant increase in consumer spending.\",\n",
    "    \"This was driven by strong job growth and rising wages.\",\n",
    "    \"However, inflation remained a concern.\"\n",
    "]\n",
    "embeddings = model.encode(text_chunks)\n",
    "\n",
    "# Step 2: Create a vector index\n",
    "embedding_dim = embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(embedding_dim)\n",
    "index.add(np.array(embeddings))\n",
    "\n",
    "# Step 3: Perform similarity search\n",
    "query = \"What caused the increase in consumer spending?\"\n",
    "query_embedding = model.encode([query])\n",
    "k = 2\n",
    "distances, indices = index.search(query_embedding, k)\n",
    "\n",
    "# Step 4: Retrieve results\n",
    "for i, idx in enumerate(indices[0]):\n",
    "    print(f\"Result {i + 1}: {text_chunks[idx]} (Distance: {distances[0][i]})\")\n",
    "\n",
    "# Step 5: Save the index\n",
    "faiss.write_index(index, \"vector_index.faiss\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Output Example**\n",
    "For the query `\"What caused the increase in consumer spending?\"`, the output might look like:\n",
    "\n",
    "```\n",
    "Result 1: This was driven by strong job growth and rising wages. (Distance: 0.123)\n",
    "Result 2: In Q2 2023, the Federal Reserve reported a significant increase in consumer spending. (Distance: 0.456)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "By following these steps, you can create a vector index from text chunks and use it for efficient similarity search or retrieval tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46917fe7-d32f-4d81-97e2-65bbec90c15c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
