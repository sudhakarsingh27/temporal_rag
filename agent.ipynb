{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a ShoeBot Sales Agent using LangGraph and Graphiti\n",
    "\n",
    "The following example demonstrates building an agent using LangGraph. Graphiti is used to personalize agent responses based on information learned from prior conversations. Additionally, a database of products is loaded into the Graphiti graph, enabling the agent to speak to these products.\n",
    "\n",
    "The agent implements:\n",
    "- persistance of new chat turns to Graphiti and recall of relevant Facts using the most recent message.\n",
    "- a tool for querying Graphiti for shoe information\n",
    "- an in-memory MemorySaver to maintain agent state.\n",
    "\n",
    "## Install dependencies\n",
    "```shell\n",
    "pip install graphiti-core langchain-openai langgraph ipywidgets\n",
    "```\n",
    "\n",
    "Ensure that you've followed the Graphiti installation instructions. In particular, installation of `neo4j`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import asyncio\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import uuid\n",
    "from contextlib import suppress\n",
    "from datetime import datetime, timezone\n",
    "from pathlib import Path\n",
    "from typing import Annotated\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Image, display\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_logging():\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.ERROR)\n",
    "    console_handler = logging.StreamHandler(sys.stdout)\n",
    "    console_handler.setLevel(logging.INFO)\n",
    "    formatter = logging.Formatter('%(name)s - %(levelname)s - %(message)s')\n",
    "    console_handler.setFormatter(formatter)\n",
    "    logger.addHandler(console_handler)\n",
    "    return logger\n",
    "\n",
    "\n",
    "logger = setup_logging()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangSmith integration (Optional)\n",
    "\n",
    "If you'd like to trace your agent using LangSmith, ensure that you have a `LANGSMITH_API_KEY` set in your environment.\n",
    "\n",
    "Then set `os.environ['LANGCHAIN_TRACING_V2'] = 'false'` to `true`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['LANGCHAIN_TRACING_V2'] = 'false'\n",
    "os.environ['LANGCHAIN_PROJECT'] = 'Graphiti LangGraph Tutorial'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure local LLM coz OpenAI rate limits suck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface.llms import HuggingFacePipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from huggingface_hub import login\n",
    "from typing import Any, Dict\n",
    "\n",
    "class CustomHuggingFacePipeline(HuggingFacePipeline):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "    async def generate_response(self, prompt: str, **kwargs) -> Dict[str, Any]:\n",
    "        result = self.invoke(prompt, **kwargs)\n",
    "        return {\"response\": result}\n",
    "\n",
    "token = os.environ[\"HF_API_KEY\"]\n",
    "login(token)\n",
    "model_id = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    token=token,\n",
    ")\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=20)\n",
    "hf_llm = CustomHuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "### Code to test the local LLM inference\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step, seriously are you \"\"\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "chain = prompt | hf_llm\n",
    "\n",
    "question = \"What is electroencephalography?\"\n",
    "\n",
    "response = chain.invoke({\"question\": question})\n",
    "print(len(response) - len(template), response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Graphiti\n",
    "\n",
    "Ensure that you have `neo4j` running and a database created. Ensure that you've configured the following in your environment.\n",
    "\n",
    "```bash\n",
    "NEO4J_URI=\n",
    "NEO4J_USER=\n",
    "NEO4J_PASSWORD=\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Graphiti\n",
    "\n",
    "from graphiti_core import Graphiti\n",
    "from graphiti_core.edges import EntityEdge\n",
    "from graphiti_core.nodes import EpisodeType\n",
    "from graphiti_core.utils.maintenance.graph_data_operations import clear_data\n",
    "\n",
    "neo4j_uri = os.environ.get('NEO4J_URI', 'bolt://localhost:7687')\n",
    "neo4j_user = os.environ.get('NEO4J_USER', 'neo4j')\n",
    "neo4j_password = os.environ.get('NEO4J_PASSWORD', 'password')\n",
    "\n",
    "client = Graphiti(\n",
    "    neo4j_uri,\n",
    "    neo4j_user,\n",
    "    neo4j_password,\n",
    "    # llm_client=hf_llm,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating a database schema \n",
    "\n",
    "The following is only required for the first run of this notebook or when you'd like to start your database over.\n",
    "\n",
    "**IMPORTANT**: `clear_data` is destructive and will wipe your entire database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: This will clear the database\n",
    "await clear_data(client.driver)\n",
    "await client.build_indices_and_constraints()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Shoe Data into the Graph\n",
    "\n",
    "Load several shoe and related products into the Graphiti. This may take a while.\n",
    "\n",
    "\n",
    "**IMPORTANT**: This only needs to be done once. If you run `clear_data` you'll need to rerun this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def ingest_products_data(client: Graphiti):\n",
    "    script_dir = Path.cwd().parent\n",
    "    json_file_path = script_dir / 'graphiti/examples/data' / 'manybirds_products.json'\n",
    "\n",
    "    with open(json_file_path) as file:\n",
    "        products = json.load(file)['products']\n",
    "\n",
    "    for i, product in enumerate(products):\n",
    "        await client.add_episode(\n",
    "            name=product.get('title', f'Product {i}'),\n",
    "            episode_body=str({k: v for k, v in product.items() if k != 'images'}),\n",
    "            source_description='ManyBirds products',\n",
    "            source=EpisodeType.json,\n",
    "            reference_time=datetime.now(timezone.utc),\n",
    "        )\n",
    "\n",
    "\n",
    "await ingest_products_data(client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI Rate Limit sucks, so quick fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "RateLimitError",
     "evalue": "Rate limit exceeded. Please try again later.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/graphiti_core/llm_client/openai_client.py\u001b[0m in \u001b[0;36m_generate_response\u001b[0;34m(self, messages, response_model, max_tokens)\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m             response = await self.client.beta.chat.completions.parse(\n\u001b[0m\u001b[1;32m    105\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mDEFAULT_MODEL\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/openai/resources/beta/chat/completions.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, messages, model, audio, response_format, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, seed, service_tier, stop, store, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         return await self._post(\n\u001b[0m\u001b[1;32m    436\u001b[0m             \u001b[0;34m\"/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1848\u001b[0m         )\n\u001b[0;32m-> 1849\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, stream, stream_cls, remaining_retries)\u001b[0m\n\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1543\u001b[0;31m         return await self._request(\n\u001b[0m\u001b[1;32m   1544\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, stream, stream_cls, retries_taken)\u001b[0m\n\u001b[1;32m   1628\u001b[0m                 \u001b[0;32mawait\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1629\u001b[0;31m                 return await self._retry_request(\n\u001b[0m\u001b[1;32m   1630\u001b[0m                     \u001b[0minput_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_retry_request\u001b[0;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1676\u001b[0;31m         return await self._request(\n\u001b[0m\u001b[1;32m   1677\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, stream, stream_cls, retries_taken)\u001b[0m\n\u001b[1;32m   1628\u001b[0m                 \u001b[0;32mawait\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1629\u001b[0;31m                 return await self._retry_request(\n\u001b[0m\u001b[1;32m   1630\u001b[0m                     \u001b[0minput_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_retry_request\u001b[0;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1676\u001b[0;31m         return await self._request(\n\u001b[0m\u001b[1;32m   1677\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, stream, stream_cls, retries_taken)\u001b[0m\n\u001b[1;32m   1643\u001b[0m             \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1644\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1645\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-T0edeuNzYdPaRB1v5tsGNSks on tokens per min (TPM): Limit 200000, Used 199001, Requested 1680. Please try again in 204ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8264/2003958301.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;31m# Run with increased timeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m \u001b[0;32mawait\u001b[0m \u001b[0mingest_products_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_8264/2003958301.py\u001b[0m in \u001b[0;36mingest_products_data\u001b[0;34m(client)\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMAX_CONCURRENT_REQUESTS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtasks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mMAX_CONCURRENT_REQUESTS\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0;32mawait\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m         \u001b[0;32mawait\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBASE_DELAY\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Inter-batch delay\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/tenacity/asyncio/__init__.py\u001b[0m in \u001b[0;36masync_wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    187\u001b[0m             \u001b[0mcopy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m             \u001b[0masync_wrapped\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatistics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatistics\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0;31m# Preserve attributes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/tenacity/asyncio/__init__.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0mretry_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRetryCallState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry_object\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0mdo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDoAttempt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/tenacity/asyncio/__init__.py\u001b[0m in \u001b[0;36miter\u001b[0;34m(self, retry_state)\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maction\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/tenacity/_utils.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(rs)\u001b[0m\n\u001b[1;32m    396\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_post_retry_check_actions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretry_state\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"RetryCallState\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_explicit_retry\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretry_run_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 398\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_action_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutcome\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    399\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    449\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 451\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    404\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m                 \u001b[0;31m# Break a reference cycle with the exception in self._exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/tenacity/asyncio/__init__.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDoAttempt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# noqa: B902\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                     \u001b[0mretry_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_8264/2003958301.py\u001b[0m in \u001b[0;36mthrottled_add_episode\u001b[0;34m(client, product, idx)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mthrottled_add_episode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mGraphiti\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproduct\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32masync\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0msemaphore\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Enforce concurrency limit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         await client.add_episode(\n\u001b[0m\u001b[1;32m     41\u001b[0m             \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproduct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'title'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'Product {idx}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mepisode_body\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mproduct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'images'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/graphiti_core/graphiti.py\u001b[0m in \u001b[0;36madd_episode\u001b[0;34m(self, name, episode_body, source_description, reference_time, source, group_id, uuid, update_communities)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0madd_episode_bulk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbulk_episodes\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mRawEpisode\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup_id\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/graphiti_core/graphiti.py\u001b[0m in \u001b[0;36madd_episode\u001b[0;34m(self, name, episode_body, source_description, reference_time, source, group_id, uuid, update_communities)\u001b[0m\n\u001b[1;32m    354\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Extracted nodes: {[(n.name, n.uuid) for n in extracted_nodes]}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m             (mentioned_nodes, uuid_map), extracted_edges = await semaphore_gather(\n\u001b[0m\u001b[1;32m    357\u001b[0m                 resolve_extracted_nodes(\n\u001b[1;32m    358\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllm_client\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/graphiti_core/helpers.py\u001b[0m in \u001b[0;36msemaphore_gather\u001b[0;34m(max_coroutines, *coroutines)\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mcoroutine\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_wrap_coroutine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoroutine\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcoroutine\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcoroutines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/graphiti_core/helpers.py\u001b[0m in \u001b[0;36m_wrap_coroutine\u001b[0;34m(coroutine)\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_wrap_coroutine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoroutine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32masync\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0msemaphore\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mcoroutine\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_wrap_coroutine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoroutine\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcoroutine\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcoroutines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/graphiti_core/utils/maintenance/node_operations.py\u001b[0m in \u001b[0;36mresolve_extracted_nodes\u001b[0;34m(llm_client, extracted_nodes, existing_nodes_lists, episode, previous_episodes)\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0mresolved_nodes\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mEntityNode\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m     results: list[tuple[EntityNode, dict[str, str]]] = list(\n\u001b[0;32m--> 225\u001b[0;31m         await semaphore_gather(\n\u001b[0m\u001b[1;32m    226\u001b[0m             *[\n\u001b[1;32m    227\u001b[0m                 resolve_extracted_node(\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/graphiti_core/helpers.py\u001b[0m in \u001b[0;36msemaphore_gather\u001b[0;34m(max_coroutines, *coroutines)\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mcoroutine\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_wrap_coroutine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoroutine\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcoroutine\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcoroutines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/graphiti_core/helpers.py\u001b[0m in \u001b[0;36m_wrap_coroutine\u001b[0;34m(coroutine)\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_wrap_coroutine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoroutine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32masync\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0msemaphore\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mcoroutine\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_wrap_coroutine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoroutine\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcoroutine\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcoroutines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/graphiti_core/utils/maintenance/node_operations.py\u001b[0m in \u001b[0;36mresolve_extracted_node\u001b[0;34m(llm_client, extracted_node, existing_nodes, episode, previous_episodes)\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mexisting_node\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muuid\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0muuid\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m             summary_response = await llm_client.generate_response(\n\u001b[0m\u001b[1;32m    300\u001b[0m                 prompt_library.summarize_nodes.summarize_pair(\n\u001b[1;32m    301\u001b[0m                     \u001b[0;34m{\u001b[0m\u001b[0;34m'node_summaries'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mextracted_node\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexisting_node\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/graphiti_core/llm_client/openai_client.py\u001b[0m in \u001b[0;36mgenerate_response\u001b[0;34m(self, messages, response_model, max_tokens)\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mretry_count\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMAX_RETRIES\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m                 \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mRateLimitError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRefusalError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/graphiti_core/llm_client/openai_client.py\u001b[0m in \u001b[0;36m_generate_response\u001b[0;34m(self, messages, response_model, max_tokens)\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Output length exceeded max tokens {self.max_tokens}: {e}'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mopenai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRateLimitError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRateLimitError\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Error in generating LLM response: {e}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRateLimitError\u001b[0m: Rate limit exceeded. Please try again later."
     ]
    }
   ],
   "source": [
    "import time\n",
    "from pathlib import Path\n",
    "import json\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "# Rate limiting config\n",
    "RATE_LIMIT_TPM = 200000  # OpenAI's limit for gpt-4o-mini\n",
    "TOKEN_BUFFER = 0.1  # 10% safety margin\n",
    "MAX_TOKENS_PER_MINUTE = RATE_LIMIT_TPM * (1 - TOKEN_BUFFER)\n",
    "\n",
    "async def ingest_products_data(client: Graphiti):\n",
    "    script_dir = Path.cwd().parent\n",
    "    json_file_path = script_dir / 'temporal_rag/graphiti/examples/data' / 'manybirds_products.json'\n",
    "\n",
    "    with open(json_file_path) as file:\n",
    "        products = json.load(file)['products']\n",
    "\n",
    "    tokens_used = 0\n",
    "    start_time = time.time()\n",
    "\n",
    "    for i, product in enumerate(products):\n",
    "        # Estimate tokens for this request (simplified example)\n",
    "        episode_body = str({k: v for k, v in product.items() if k != 'images'})\n",
    "        estimated_tokens = len(episode_body) // 4  # Approximate 4 chars/token\n",
    "\n",
    "        # Throttle check\n",
    "        if tokens_used + estimated_tokens > MAX_TOKENS_PER_MINUTE:\n",
    "            elapsed = time.time() - start_time\n",
    "            sleep_time = max(60 - elapsed, 0)\n",
    "            print(f\"Approaching limit, sleeping for {sleep_time:.1f}s\")\n",
    "            await asyncio.sleep(sleep_time)\n",
    "            tokens_used = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "        # Make the API call\n",
    "        await client.add_episode(\n",
    "            name=product.get('title', f'Product {i}'),\n",
    "            episode_body=episode_body,\n",
    "            source_description='ManyBirds products',\n",
    "            source=EpisodeType.json,\n",
    "            reference_time=datetime.now(timezone.utc),\n",
    "        )\n",
    "\n",
    "        tokens_used += estimated_tokens\n",
    "\n",
    "        # Progress tracking\n",
    "        if i % 10 == 0:\n",
    "            print(f\"Processed {i+1}/{len(products)} products\")\n",
    "\n",
    "    print(f\"Ingested {len(products)} products successfully\")\n",
    "\n",
    "# To run in Jupyter:\n",
    "await ingest_products_data(client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a user node in the Graphiti graph\n",
    "\n",
    "In your own app, this step could be done later once the user has identified themselves and made their sales intent known. We do this here so we can configure the agent with the user's `node_uuid`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphiti_core.search.search_config_recipes import NODE_HYBRID_SEARCH_EPISODE_MENTIONS\n",
    "\n",
    "user_name = 'jess'\n",
    "\n",
    "await client.add_episode(\n",
    "    name='User Creation',\n",
    "    episode_body=(f'{user_name} is interested in buying a pair of shoes'),\n",
    "    source=EpisodeType.text,\n",
    "    reference_time=datetime.now(timezone.utc),\n",
    "    source_description='SalesBot',\n",
    ")\n",
    "\n",
    "# let's get Jess's node uuid\n",
    "nl = await client._search(user_name, NODE_HYBRID_SEARCH_EPISODE_MENTIONS)\n",
    "\n",
    "user_node_uuid = nl.nodes[0].uuid\n",
    "\n",
    "# and the ManyBirds node uuid\n",
    "nl = await client._search('ManyBirds', NODE_HYBRID_SEARCH_EPISODE_MENTIONS)\n",
    "manybirds_node_uuid = nl.nodes[0].uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edges_to_facts_string(entities: list[EntityEdge]):\n",
    "    return '-' + '\\n- '.join([edge.fact for edge in entities])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage, SystemMessage\n",
    "from langchain_core.tools import tool\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import END, START, StateGraph, add_messages\n",
    "from langgraph.prebuilt import ToolNode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `get_shoe_data` Tool\n",
    "\n",
    "The agent will use this to search the Graphiti graph for information about shoes. We center the search on the `manybirds_node_uuid` to ensure we rank shoe-related data over user data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "async def get_shoe_data(query: str) -> str:\n",
    "    \"\"\"Search the graphiti graph for information about shoes\"\"\"\n",
    "    edge_results = await client.search(\n",
    "        query,\n",
    "        center_node_uuid=manybirds_node_uuid,\n",
    "        num_results=10,\n",
    "    )\n",
    "    return edges_to_facts_string(edge_results)\n",
    "\n",
    "\n",
    "tools = [get_shoe_data]\n",
    "tool_node = ToolNode(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model='gpt-4o-mini', temperature=0).bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [ToolMessage(content=\"-jess is interested in buying a pair of Men's SuperLight Wool Runners - Dark Grey (Medium Grey Sole)\\n- jess is interested in buying a pair of TinyBirds Wool Runners - Little Kids - Natural Black (Blizzard Sole)\\n- jess is interested in buying a pair of Anytime No Show Sock - Rugged Beige\\n- jess is interested in buying a pair of Women's Tree Breezers Knit - Rugged Beige (Hazy Beige Sole)\\n- jess is interested in buying a pair of Men's Couriers - Natural Black/Basin Blue (Blizzard Sole)\\n- Men's Couriers are classified as Shoes.\\n- Men's Couriers have a blizzard sole.\\n- The product 'Women's Tree Breezers Knit - Rugged Beige (Hazy Beige Sole)' has the tag 'YCRF_womens-move-shoes-half-sizes'.\\n- The product 'Women's Tree Breezers Knit - Rugged Beige (Hazy Beige Sole)' has the tag 'shoprunner'.\\n- The product 'Women's Tree Breezers Knit - Rugged Beige (Hazy Beige Sole)' has the tag 'YGroup_ygroup_womens-tree-breezers'.\", name='get_shoe_data', tool_call_id='call_yCVcerP8t8KM0mwrKT1YLkv2')]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the tool node\n",
    "await tool_node.ainvoke({'messages': [await llm.ainvoke('wool shoes')]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chatbot Function Explanation\n",
    "\n",
    "The chatbot uses Graphiti to provide context-aware responses in a shoe sales scenario. Here's how it works:\n",
    "\n",
    "1. **Context Retrieval**: It searches the Graphiti graph for relevant information based on the latest message, using the user's node as the center point. This ensures that user-related facts are ranked higher than other information in the graph.\n",
    "\n",
    "2. **System Message**: It constructs a system message incorporating facts from Graphiti, setting the context for the AI's response.\n",
    "\n",
    "3. **Knowledge Persistence**: After generating a response, it asynchronously adds the interaction to the Graphiti graph, allowing future queries to reference this conversation.\n",
    "\n",
    "This approach enables the chatbot to maintain context across interactions and provide personalized responses based on the user's history and preferences stored in the Graphiti graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    user_name: str\n",
    "    user_node_uuid: str\n",
    "\n",
    "\n",
    "async def chatbot(state: State):\n",
    "    facts_string = None\n",
    "    if len(state['messages']) > 0:\n",
    "        last_message = state['messages'][-1]\n",
    "        graphiti_query = f'{\"SalesBot\" if isinstance(last_message, AIMessage) else state[\"user_name\"]}: {last_message.content}'\n",
    "        # search graphiti using Jess's node uuid as the center node\n",
    "        # graph edges (facts) further from the Jess node will be ranked lower\n",
    "        edge_results = await client.search(\n",
    "            graphiti_query, center_node_uuid=state['user_node_uuid'], num_results=5\n",
    "        )\n",
    "        facts_string = edges_to_facts_string(edge_results)\n",
    "\n",
    "    system_message = SystemMessage(\n",
    "        content=f\"\"\"You are a skillfull shoe salesperson working for ManyBirds. Review information about the user and their prior conversation below and respond accordingly.\n",
    "        Keep responses short and concise. And remember, always be selling (and helpful!)\n",
    "\n",
    "        Things you'll need to know about the user in order to close a sale:\n",
    "        - the user's shoe size\n",
    "        - any other shoe needs? maybe for wide feet?\n",
    "        - the user's preferred colors and styles\n",
    "        - their budget\n",
    "\n",
    "        Ensure that you ask the user for the above if you don't already know.\n",
    "\n",
    "        Facts about the user and their conversation:\n",
    "        {facts_string or 'No facts about the user and their conversation'}\"\"\"\n",
    "    )\n",
    "\n",
    "    messages = [system_message] + state['messages']\n",
    "\n",
    "    response = await llm.ainvoke(messages)\n",
    "\n",
    "    # add the response to the graphiti graph.\n",
    "    # this will allow us to use the graphiti search later in the conversation\n",
    "    # we're doing async here to avoid blocking the graph execution\n",
    "    asyncio.create_task(\n",
    "        client.add_episode(\n",
    "            name='Chatbot Response',\n",
    "            episode_body=f\"{state['user_name']}: {state['messages'][-1]}\\nSalesBot: {response.content}\",\n",
    "            source=EpisodeType.message,\n",
    "            reference_time=datetime.now(timezone.utc),\n",
    "            source_description='Chatbot',\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return {'messages': [response]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the Agent\n",
    "\n",
    "This section sets up the Agent's LangGraph graph:\n",
    "\n",
    "1. **Graph Structure**: It defines a graph with nodes for the agent (chatbot) and tools, connected in a loop.\n",
    "\n",
    "2. **Conditional Logic**: The `should_continue` function determines whether to end the graph execution or continue to the tools node based on the presence of tool calls.\n",
    "\n",
    "3. **Memory Management**: It uses a MemorySaver to maintain conversation state across turns. This is in addition to using Graphiti for facts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_builder = StateGraph(State)\n",
    "\n",
    "memory = MemorySaver()\n",
    "\n",
    "\n",
    "# Define the function that determines whether to continue or not\n",
    "async def should_continue(state, config):\n",
    "    messages = state['messages']\n",
    "    last_message = messages[-1]\n",
    "    # If there is no function call, then we finish\n",
    "    if not last_message.tool_calls:\n",
    "        return 'end'\n",
    "    # Otherwise if there is, we continue\n",
    "    else:\n",
    "        return 'continue'\n",
    "\n",
    "\n",
    "graph_builder.add_node('agent', chatbot)\n",
    "graph_builder.add_node('tools', tool_node)\n",
    "\n",
    "graph_builder.add_edge(START, 'agent')\n",
    "graph_builder.add_conditional_edges('agent', should_continue, {'continue': 'tools', 'end': END})\n",
    "graph_builder.add_edge('tools', 'agent')\n",
    "\n",
    "graph = graph_builder.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our LangGraph agent graph is illustrated below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with suppress(Exception):\n",
    "    display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the Agent\n",
    "\n",
    "Let's test the agent with a single call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='What sizes do the TinyBirds Wool Runners in Natural Black come in?', additional_kwargs={}, response_metadata={}, id='c23f0964-9191-49c7-a39f-547611555732'),\n",
       "  AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_SUv2heqDusw1jVo4SRLdKaw5', 'function': {'arguments': '{\"query\":\"TinyBirds Wool Runners - Little Kids - Natural Black\"}', 'name': 'get_shoe_data'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 28, 'prompt_tokens': 303, 'total_tokens': 331, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_72ed7ab54c', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-b33bcd23-2dc3-4def-be4f-6ff73f82ebc0-0', tool_calls=[{'name': 'get_shoe_data', 'args': {'query': 'TinyBirds Wool Runners - Little Kids - Natural Black'}, 'id': 'call_SUv2heqDusw1jVo4SRLdKaw5', 'type': 'tool_call'}], usage_metadata={'input_tokens': 303, 'output_tokens': 28, 'total_tokens': 331, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       "  ToolMessage(content=\"-jess is interested in buying a pair of TinyBirds Wool Runners - Little Kids - Natural Black (Blizzard Sole)\\n- jess is interested in buying a pair of Men's SuperLight Wool Runners - Dark Grey (Medium Grey Sole)\\n- jess is interested in buying a pair of Men's Couriers - Natural Black/Basin Blue (Blizzard Sole)\\n- Men's Couriers are available in Natural Black.\\n- The product 'Women's Tree Breezers Knit - Rugged Beige (Hazy Beige Sole)' has the tag 'shoprunner'.\\n- The product 'Women's Tree Breezers Knit - Rugged Beige (Hazy Beige Sole)' has the tag 'YCRF_womens-move-shoes-half-sizes'.\\n- The product 'Women's Tree Breezers Knit - Rugged Beige (Hazy Beige Sole)' has the tag 'YGroup_ygroup_womens-tree-breezers'.\\n- The product 'Women's Tree Breezers Knit - Rugged Beige (Hazy Beige Sole)' has a variant with ID '40832464322640'.\\n- The product 'Women's Tree Breezers Knit - Rugged Beige (Hazy Beige Sole)' has a variant with ID '40832464453712'.\\n- The product 'Women's Tree Breezers Knit - Rugged Beige (Hazy Beige Sole)' has a variant with ID '40832464683088'.\", name='get_shoe_data', id='0388d740-897f-44ae-bce2-878a74412b1b', tool_call_id='call_SUv2heqDusw1jVo4SRLdKaw5'),\n",
       "  AIMessage(content='The TinyBirds Wool Runners in Natural Black are available in various sizes for little kids. If you let me know the specific size you need, I can help you check availability! Also, do you have any other preferences or needs, like color or style?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 55, 'prompt_tokens': 642, 'total_tokens': 697, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_72ed7ab54c', 'finish_reason': 'stop', 'logprobs': None}, id='run-ef31952e-5d9f-4dc0-afec-38334927e3ac-0', usage_metadata={'input_tokens': 642, 'output_tokens': 55, 'total_tokens': 697, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})],\n",
       " 'user_name': 'jess',\n",
       " 'user_node_uuid': '84f3a682-46f0-4c53-89d5-bcb2c8eafc08'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await graph.ainvoke(\n",
    "    {\n",
    "        'messages': [\n",
    "            {\n",
    "                'role': 'user',\n",
    "                'content': 'What sizes do the TinyBirds Wool Runners in Natural Black come in?',\n",
    "            }\n",
    "        ],\n",
    "        'user_name': user_name,\n",
    "        'user_node_uuid': user_node_uuid,\n",
    "    },\n",
    "    config={'configurable': {'thread_id': uuid.uuid4().hex}},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viewing the Graph\n",
    "\n",
    "At this stage, the graph would look something like this. The `jess` node is `INTERESTED_IN` the `TinyBirds Wool Runner` node. The image below was generated using Neo4j Desktop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'tinybirds-jess.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_16544/2259909876.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'tinybirds-jess.png'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m850\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib/python3/dist-packages/IPython/core/display.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, url, filename, format, embed, width, height, retina, unconfined, metadata)\u001b[0m\n\u001b[1;32m   1229\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretina\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mretina\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1230\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munconfined\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munconfined\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1231\u001b[0;31m         super(Image, self).__init__(data=data, url=url, filename=filename, \n\u001b[0m\u001b[1;32m   1232\u001b[0m                 metadata=metadata)\n\u001b[1;32m   1233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/IPython/core/display.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, url, filename, metadata)\u001b[0m\n\u001b[1;32m    635\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 637\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    638\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/IPython/core/display.py\u001b[0m in \u001b[0;36mreload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1261\u001b[0m         \u001b[0;34m\"\"\"Reload the raw data from file or URL.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1262\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1263\u001b[0;31m             \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1264\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretina\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1265\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retina_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/IPython/core/display.py\u001b[0m in \u001b[0;36mreload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    660\u001b[0m         \u001b[0;34m\"\"\"Reload the raw data from file or URL.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 662\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_flags\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    663\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    664\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'tinybirds-jess.png'"
     ]
    }
   ],
   "source": [
    "display(Image(filename='tinybirds-jess.png', width=850))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the Agent interactively\n",
    "\n",
    "The following code will run the agent in an event loop. Just enter a message into the box and click submit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49841bfa2efe438f98e40f40a26b610c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Text(value='', placeholder='Type your message here...'), Button(description='Send', style=Butto…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "conversation_output = widgets.Output()\n",
    "config = {'configurable': {'thread_id': uuid.uuid4().hex}}\n",
    "user_state = {'user_name': user_name, 'user_node_uuid': user_node_uuid}\n",
    "\n",
    "\n",
    "async def process_input(user_state: State, user_input: str):\n",
    "    conversation_output.append_stdout(f'\\nUser: {user_input}\\n')\n",
    "    conversation_output.append_stdout('\\nAssistant: ')\n",
    "\n",
    "    graph_state = {\n",
    "        'messages': [{'role': 'user', 'content': user_input}],\n",
    "        'user_name': user_state['user_name'],\n",
    "        'user_node_uuid': user_state['user_node_uuid'],\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        async for event in graph.astream(\n",
    "            graph_state,\n",
    "            config=config,\n",
    "        ):\n",
    "            for value in event.values():\n",
    "                if 'messages' in value:\n",
    "                    last_message = value['messages'][-1]\n",
    "                    if isinstance(last_message, AIMessage) and isinstance(\n",
    "                        last_message.content, str\n",
    "                    ):\n",
    "                        conversation_output.append_stdout(last_message.content)\n",
    "    except Exception as e:\n",
    "        conversation_output.append_stdout(f'Error: {e}')\n",
    "\n",
    "\n",
    "def on_submit(b):\n",
    "    user_input = input_box.value\n",
    "    input_box.value = ''\n",
    "    asyncio.create_task(process_input(user_state, user_input))\n",
    "\n",
    "\n",
    "input_box = widgets.Text(placeholder='Type your message here...')\n",
    "submit_button = widgets.Button(description='Send')\n",
    "submit_button.on_click(on_submit)\n",
    "\n",
    "conversation_output.append_stdout('Asssistant: Hello, how can I help you find shoes today?')\n",
    "\n",
    "display(widgets.VBox([input_box, submit_button, conversation_output]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
